{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import ast\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md\n",
    "import spacy\n",
    "nlp =  spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = pd.read_csv('../data/ted_main.csv')\n",
    "df_transcripts = pd.read_csv('../data/transcripts.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} talks, with {} duplicate titles, and {} speakers that have more than one talk.\".format(\n",
    "        len(df_metadata),\n",
    "        df_metadata.duplicated('title').sum(),\n",
    "        df_metadata.duplicated('main_speaker').sum()\n",
    "     ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# 1 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on:\n",
    "\n",
    "* tags.\n",
    "* description\n",
    "* (mabe title? (same as name, except that name also has the name of the speaker))\n",
    "\n",
    "But keep in mind that there are other super interesting columns, for example: stuff related with popularity (ratings, comments) and talks that a particular talk is related to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most talks have 4-7 tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata['number_tags'] = df_metadata.tags.apply(lambda x: len(ast.literal_eval(x)))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(16,3))\n",
    "sns.countplot(x='number_tags',data=df_metadata,ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most popular tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list_of_list_of_words(l):\n",
    "    \"\"\"The tags column returns a list of lists of strings, this function just turns it to a single list.\n",
    "    It is a bit annoying cause python usually loops on chars not words, but I found this dark magic here:\n",
    "    https://stackoverflow.com/questions/52981376/flatten-list-of-list-of-strings\n",
    "    \"\"\"\n",
    "    return [inner for item in df_metadata.tags for inner in ast.literal_eval(item)] \n",
    "\n",
    "df_all_tags = pd.Series(flatten_list_of_list_of_words(df_metadata.tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###(\n",
    "# Just to make the issue clear:\n",
    "tags_test = df_metadata.tags\n",
    "tags = flatten_list_of_list_of_words(df_metadata.tags)\n",
    "print(tags[0],'vs',tags_test[0])\n",
    "print(len(np.unique(tags)),'vs',len(np.unique(tags_test)))\n",
    "###)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_tags.value_counts().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly a useless plot, but that people seem to enjoy, cause it's kinda of cool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (' ').join(df_all_tags)\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "wc = WordCloud(background_color=\"white\",width=1800,height=600).generate(text)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a group of tags that are way more used than others (that we might ignore)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(16,3))\n",
    "sns.violinplot(df_all_tags.value_counts(),ax=ax,color='C1')\n",
    "ax.set_xlabel('Number of times a tag was used')\n",
    "\n",
    "q25,q50,q75 = np.quantile(df_all_tags.value_counts(),[0.25,0.50,0.75])\n",
    "#ax.axvline(q25,color='k')\n",
    "#ax.axvline(q50,color='k')\n",
    "#ax.axvline(q75,color='k');\n",
    "\n",
    "print('25% of the tags are used less than {} times in the entire data set, 50% {} and 75% {} times)'.format(q25,q50,q75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags analysis I: K-means\n",
    "\n",
    "Can we group this tags into broader groups (and end up with just a couple of tags)? \n",
    "\n",
    "   1. Take a pre-trainned embedding (from spacy, not sure what is the model, but it has a dimmention of 96, not 300 like Glove).\n",
    "   2. Use PCA/t-sne/... to diminish the dimensions of the vectors from the embeddings.\n",
    "   3. See if we can group it using k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags = df_all_tags.value_counts().rename_axis('tags').reset_index(name='counts')\n",
    "df_tags['has_vector'] = df_tags.tags.apply(lambda x: nlp(x).has_vector)\n",
    "df_tags['word_vector'] = df_tags.tags.apply(lambda x: nlp(x).vector)\n",
    "df_tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All worlds exist in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags.has_vector.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA**\n",
    "\n",
    "I randomly choose to keep 5 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in df_tags.word_vector for item in sublist]\n",
    "X = np.reshape(flat_list,(len(df_tags),-1))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check to make sure I didn't swap the dimentions of the X matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df_tags.word_vector[42] - X[42]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "X_PCA = pca.fit_transform(X)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the K-means, we have to chose a number of classes before running the algorith. A way to try to come up with the optimal number of classes (k) is using the elbow-method (k vs mean distance of the points to the centroid to which they belong to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I just coppied this from here: https://pythonprogramminglanguage.com/kmeans-elbow-method/\n",
    "\n",
    "distortions = []\n",
    "K = range(1,50,2)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(X_PCA)\n",
    "    distortions.append(sum(np.min(cdist(X_PCA, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X_PCA.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we might skip the PCA all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "K = range(1,50,2)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(X)\n",
    "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem that there is a neat and smallish number of clusters of worlds, that would maybe point towards the same topic (e.g. childreen, education, play vs technology, computers, ai...). So **k-means doesn't seem to be that interesting to cluster ted talks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags analysis II: Network analysis\n",
    "\n",
    "Here I'm basing this work in this post about analysing a [entities relations in news](https://towardsdatascience.com/building-a-social-network-from-the-news-using-graph-theory-by-marcell-ferencz-9155d314e77f).\n",
    "\n",
    "The original articule says:\n",
    "\n",
    "> The fundamental premise behind building our social network will be two-fold and quite simple:\n",
    "> 1. If two people are mentioned in the same article, they are friends.\n",
    "> 2. The more articles mention the same two people, the closer they are as friends.\n",
    "\n",
    "\n",
    "Which I'm going to addapt for this example by:\n",
    "\n",
    "1. If two TED talks have the same tag, these talks are related (aka friends, but that's weird for talks...)\n",
    "2. The more tags talks have in common, the stronger their relation is (which is still kind of weird for talks)\n",
    "\n",
    "Notice here that I'm ignoring the actual content of the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all_tags.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_videos_with_same_tag(df):\n",
    "    links = []\n",
    "    for tag in tqdm(df_all_tags.unique()):\n",
    "        #videos_with_tag = df.title[[i for i,tag_list in enumerate(df.tags) if tag in tag_list]]\n",
    "        #all_comb_2_videos = list(combinations(videos_with_tag, 2))\n",
    "        idx_videos_with_tag = [i for i,tag_list in enumerate(df.tags) if tag in tag_list]\n",
    "        all_comb_2_videos = list(combinations(idx_videos_with_tag, 2))\n",
    "        links.append(all_comb_2_videos)\n",
    "        \n",
    "    flat_links = [item for sublist in links for item in sublist]\n",
    "    df_links = pd.DataFrame({'TED1':np.transpose(flat_links)[0],'TED2':np.transpose(flat_links)[1]})\n",
    "    \n",
    "    return df_links\n",
    "\n",
    "\n",
    "def get_links_weight(df):\n",
    "    # This is literally coppied from the medium article. Check the original: \n",
    "    # https://towardsdatascience.com/building-a-social-network-from-the-news-using-graph-theory-by-marcell-ferencz-9155d314e77f\n",
    "    df_links = df.groupby(['TED1', 'TED2']).size().reset_index()\n",
    "    df_links.rename(columns={0: 'weight'}, inplace=True)\n",
    "    df_links = df_links[df_links['weight'] > 1]\n",
    "    df_links.reset_index(drop=True, inplace=True)\n",
    "    df_links.sort_values('weight', ascending=False)\n",
    "    return df_links\n",
    "\n",
    "def add_title_for_human_readability(df_links,df):\n",
    "    df_links['Title1'] = [df.iloc[idx_ted].title for idx_ted in tqdm(df_links.TED1)]\n",
    "    df_links['Title2'] = [df.iloc[idx_ted].title for idx_ted in tqdm(df_links.TED2)]\n",
    "    return df_links\n",
    "\n",
    "df_links = get_links_weight(connect_videos_with_same_tag(df_metadata))\n",
    "df_links = add_title_for_human_readability(df_links,df_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links.sample(10).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_links[df_links['weight']>10]\n",
    "df_plot.reset_index(inplace=True, drop=True)\n",
    "G_plot = nx.Graph()\n",
    "for link in tqdm(df_plot.index):\n",
    "    G_plot.add_edge(str(df_plot.iloc[link]['TED1']),\n",
    "                    str(df_plot.iloc[link]['TED2']),\n",
    "                    weight=df_plot.iloc[link]['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.kamada_kawai_layout(G_plot)\n",
    "nodes = G_plot.nodes()\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(15,20))\n",
    "el = nx.draw_networkx_edges(G_plot, pos, alpha=0.1, ax=axs)\n",
    "nl = nx.draw_networkx_nodes(G_plot, pos, nodelist=nodes, node_color='#FAA6FF', with_labels=True, node_size=50, ax=axs)\n",
    "ll = nx.draw_networkx_labels(G_plot, pos, font_size=10, font_family='sans-serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G_plot)\n",
    "nodes = G_plot.nodes()\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(15,20))\n",
    "el = nx.draw_networkx_edges(G_plot, pos, alpha=0.1, ax=axs)\n",
    "nl = nx.draw_networkx_nodes(G_plot, pos, nodelist=nodes, node_color='#FAA6FF', with_labels=True, node_size=50, ax=axs)\n",
    "ll = nx.draw_networkx_labels(G_plot, pos, font_size=10, font_family='sans-serif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags analysis II: can we predict the tags using the description?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are too many tags (about 400) the first step is to reduce this to about 50, by just keeping the most common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_tags = df_all_tags.value_counts().sort_values(ascending=False).rename_axis('tag',axis=0).reset_index()['tag'][:50].tolist()\n",
    "most_common_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for tag in most_common_tags:\n",
    "    labels.append([1 if tag in tags else 0 for tags in df_metadata.tags])\n",
    "    \n",
    "labels = np.transpose(labels)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make input data for the model:\n",
    "    \n",
    "    version 1: td-idf unigrams\n",
    "    version 2: td-idf bigrams\n",
    "    version 3: GloVe vectors using spacy\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',strip_accents='ascii',ngram_range=(1,1),max_features=5000)\n",
    "unigrams = vectorizer.fit_transform(df_metadata.description)\n",
    "print(unigrams.toarray().shape)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',strip_accents='ascii',ngram_range=(2,2),max_features=5000)\n",
    "bigrams = vectorizer.fit_transform(df_metadata.description)\n",
    "print(bigrams.toarray().shape)\n",
    "\n",
    "vecs = df_metadata.description.apply(lambda x: nlp(x).vector)\n",
    "vecs = np.array([i for i in vecs]) # this probably can be done better\n",
    "print(vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_uni, X_test_uni, y_train_uni, y_test_uni = train_test_split(unigrams, labels, test_size=0.33, random_state=42)\n",
    "X_train_bi, X_test_bi, y_train_bi, y_test_bi     = train_test_split(bigrams, labels, test_size=0.33, random_state=42)\n",
    "X_train_vec, X_test_vec, y_train_vec, y_test_vec = train_test_split(vecs, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Very simple model**\n",
    "\n",
    "*unigrams*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import metrics\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(5000,)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(50,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy',metrics.categorical_accuracy])\n",
    "\n",
    "model.fit(X_train_uni.toarray(), y_train_uni, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_uni.toarray(),  y_test_uni, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(5000,)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(50,activation='softmax')\n",
    "])\n",
    "\n",
    "model_bi.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy',metrics.categorical_accuracy])\n",
    "\n",
    "model_bi.fit(X_train_bi.toarray(), y_train_bi, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi.evaluate(X_test_bi.toarray(),  y_test_bi, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Glove vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vec = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(300,)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(50,activation='softmax')\n",
    "])\n",
    "\n",
    "model_vec.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy',metrics.categorical_accuracy])\n",
    "\n",
    "model_vec.fit(X_train_vec, y_train_vec, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vec.evaluate(X_test_vec,  y_test_vec, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO BE CONTINUED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Titles\n",
    "\n",
    "I'll take a bit more of a manual approach here and use something quite handy that spacy has:\n",
    "\n",
    "* [Part-of-speach tagging POS](https://spacy.io/api/annotation#pos-tagging) that says if a word is a verb, noun, adjective, etc\n",
    "* [Named Entities Recognition](https://spacy.io/api/annotation#named-entities) that recognises some words as locations, people, events etc\n",
    "\n",
    "And I'll just list the verbs, nouns, and proper nouns in the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_POS(text,pos='VERB',max_words=None):\n",
    "    ''' Other maybe interestung POS: VERB, NOUN, PROPN\n",
    "    '''\n",
    "    doc = nlp(text)\n",
    "    words = [token.lemma_ for token in doc if token.pos_ == pos]\n",
    "    df_sorted_words =  pd.Series(words).value_counts().sort_values(ascending=False)\n",
    "    df_sorted_words = df_sorted_words.rename_axis('words').reset_index().rename({0:'count'},axis=1)\n",
    "    return df_sorted_words.words.to_list()[:None]\n",
    "\n",
    "df_metadata['verbs_in_title'] = df_metadata.title.apply(lambda x: count_POS(x,pos='VERB'))\n",
    "df_metadata['nouns_in_title'] = df_metadata.title.apply(lambda x: count_POS(x,pos='NOUN'))\n",
    "df_metadata['proper_nouns_in_title'] = df_metadata.title.apply(lambda x: count_POS(x,pos='PROPN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata[['title','verbs_in_title','nouns_in_title','proper_nouns_in_title']].sample(10).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Descriptions\n",
    "\n",
    "Instead of just listing the most common words, I'll also have a look at the most common verbs and nouns. Hopefully, they'll be related with the ones on the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata['verbs_in_description'] = df_metadata.description.apply(lambda x: count_POS(x,pos='VERB',max_words=None))\n",
    "df_metadata['nouns_in_description'] = df_metadata.description.apply(lambda x: count_POS(x,pos='NOUN',max_words=None))\n",
    "df_metadata['proper_nouns_in_description'] = df_metadata.description.apply(lambda x: count_POS(x,pos='PROPN',max_words=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata[['description','verbs_in_description','nouns_in_description','proper_nouns_in_description']].sample(10).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Similarities between title and description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick look of the verbs and nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata[['title','verbs_in_title','verbs_in_description']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the cosine similarity to compare the word vectors of verbs/nouns in the title to the ones in the description, just to have an idea of how close they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_lists(list_title,list_description):\n",
    "    return nlp(' '.join(list_title)).similarity(' '.join(list_description))\n",
    "\n",
    "df_metadata['similarity_verbs'] = df_metadata.apply(lambda row: similarity_lists(row['verbs_title'],row['verbs_description']),axis=1)\n",
    "df_metadata['similarity_nouns'] = df_metadata.apply(lambda row: similarity_lists(row['nouns_title'],row['nouns_description']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata['similarity_title_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO DO \n",
    "\n",
    "put all verbs in the ground form - DONE\n",
    "maybe td-idf is useful, to see what words are more unique to each description\n",
    "think of a way to relate the two, it is list vs list -- WITH SIMILARITY\n",
    "\n",
    "see how to generate news headers from news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_most_common_words(text,include_stop_words=True,max_return=None):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    if include_stop_words:\n",
    "        words = [w.text for w in doc if w.is_alpha]\n",
    "    else:\n",
    "        words = [w.text for w in doc if w.is_alpha and not w.is_stop]\n",
    "        \n",
    "    df = pd.Series(words).value_counts().sort_values(ascending=False)\n",
    "    df = df.rename_axis('word').reset_index().rename({0:'count'},axis=1)\n",
    "\n",
    "    if max_return is None:\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        return df[:max_return].to_list()\n",
    "\n",
    "def count_most_common_POS(text,pos='VERB'):\n",
    "    ''' Other maybe interestung POS: VERB, NOUN, PROPN\n",
    "    '''\n",
    "    doc = nlp(text)\n",
    "    words = [w for token in doc if token.pos_ == pos]\n",
    "    return pd.Series(words).value_counts().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "df_test = count_most_common_words(df_metadata.description[0],include_stop_words=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
